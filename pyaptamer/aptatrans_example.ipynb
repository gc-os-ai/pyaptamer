{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c969f-0099-4e67-8cee-d0ae52fdef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pyaptamer.aptatrans.model import AptaTrans\n",
    "from pyaptamer.aptatrans.pipeline import AptaTransPipeline\n",
    "from pyaptamer.aptatrans.layers.encoder import EmbeddingConfig\n",
    "\n",
    "# auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363020ec-9c2b-4a84-8352-16835ad38d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34641b55-ffa3-4392-befb-d126c0c3475c",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8525eb88-67d4-4007-be2f-02f0cc7e0150",
   "metadata": {},
   "source": [
    "Let's initialize useful constants for both aptamers and proteins embeddings. These are taken directly from the authors repositories are they are dependent on the training data used to pretrain the transformer-based encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "apta_embedding = EmbeddingConfig(\n",
    "    n_vocabs=127,\n",
    "    n_target_vocabs=344,\n",
    "    max_len=275,\n",
    ")\n",
    "prot_embedding = EmbeddingConfig(\n",
    "    n_vocabs=715,\n",
    "    n_target_vocabs=585,\n",
    "    max_len=867,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a1dca-3cad-4047-90c5-89c4a290e2a2",
   "metadata": {},
   "source": [
    "Additionally, we need to initialize a dictionary that stores the most frequent (above-average) protein 3-mer subsequences w.r.t. training data. This will be needed to encode the target protein during aptamer generation, to make embeddings work. For simplicity, let's just use the protein frequencies stored in this .pickle file provided by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496dea4a-2f68-4ca1-9791-a5880bcfab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../pyaptamer/data/protein_word_freq.pickle', 'rb') as inf:\n",
    "    df = pickle.load(inf) # load\n",
    "    words = df.set_index('seq')['freq'].to_dict() # to dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef53da4-330b-42a9-b455-d760e65f4bc4",
   "metadata": {},
   "source": [
    "# AptaTrans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641fe00a-af15-46b3-8e7a-031e82420258",
   "metadata": {},
   "source": [
    "Now, let's initialize an instance of AptaTrans (neural network) to be used within the pipeline that leverages Apta-MCTS for aptamer recommendations. For simplicity, we keep the default parameters (e.g., number of layers, layers' dimensions, etc.) and we do not train the neural network from scratch, as it is not supported yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d875a-dc40-4100-9c79-d1390471c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "aptatrans = AptaTrans(\n",
    "    apta_embedding=apta_embedding,\n",
    "    prot_embedding=prot_embedding,\n",
    ")\n",
    "pipeline = AptaTransPipeline(\n",
    "    device=device,\n",
    "    model=aptatrans,\n",
    "    prot_words=words,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992028b9-5724-49ca-aaf9-e35dce6d3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'STEYKLVVVGADGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLLDILDTAGQEEYSAMRDQYMRTGEGFLCVFAINNTKSFEDIHHYREQIKRVKDSEDVPMVLVGNKCDLPSRTVDTKQAQDLARSYGIPFIETSAKTRQGVDDAFYTLVREIRKHKEKMSK'\n",
    "candidates = pipeline.recommend(\n",
    "    target=target,\n",
    "    n_candidates=2,\n",
    "    depth=5,\n",
    "    n_iterations=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
