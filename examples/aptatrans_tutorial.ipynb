{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pyaptamer.aptatrans import (\n",
    "    AptaTrans,\n",
    "    AptaTransEncoderLightning,\n",
    "    AptaTransLightning,\n",
    "    AptaTransPipeline,\n",
    "    EncoderPredictorConfig,\n",
    ")\n",
    "from pyaptamer.datasets import (\n",
    "    load_csv_dataset,\n",
    "    load_hf_dataset,\n",
    ")\n",
    "from pyaptamer.datasets.dataclasses import APIDataset, MaskedDataset\n",
    "from pyaptamer.experiments import Aptamer\n",
    "from pyaptamer.utils._aptatrans_utils import seq2vec\n",
    "from pyaptamer.utils._augment import augment_reverse\n",
    "from pyaptamer.utils._base import filter_words\n",
    "from pyaptamer.utils._rna import rna2vec\n",
    "\n",
    "# setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "TEST_SIZE = 0.05  # size of the test set for pretraining\n",
    "RAMDOM_STATE = 42  # for reproducibility\n",
    "\n",
    "# embeddings for pretraining\n",
    "# aptamers\n",
    "N_APTA_VOCABS = 127\n",
    "N_APTA_TARGET_VOCABS = 344\n",
    "APTA_MAX_LEN = 275\n",
    "# proteins\n",
    "N_PROT_VOCABS = 715\n",
    "N_PROT_TARGET_VOCABS = 585\n",
    "PROT_MAX_LEN = 867"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AptaTrans\n",
    "In this notebook, we present a tutorial on using AptaTrans [[1](#ref-1)] for the following tasks:\n",
    "1. **Aptamer-protein interaction (API) prediction:** given aptamer and target protein sequences, predict whether they bind or not (binary classification);\n",
    "2. **Recommend candidate aptamers:** given a target protein sequence, combine AptaTrans with Monte Carlo Tree Search (MCTS) to generate candidate aptamers that are likely to bind with the target.\n",
    "\n",
    "AptaTrans is a deep neural network based on two attention-based encoders, one for aptamers and one for proteins, and multiple convolutional layers. The encoders are employed to capture relevant features for structural representation of aptamers and proteins respectively, by pretraining them on the task of predicting the secondary structure (ss) of given sequences. After pretraining, the deep neural network may be further fine-tuned by training it on a binary classification dataset of API interactions.\n",
    "\n",
    "The tutorial is organized as follows. In [Step 1](#step-1-data-preparation), we load and pre-process the data needed for pretraining and fine-tuning. In [Step 2](#step-2-model) we initialize AptaTrans' neural network. In [Step 3](#step-3-training) we cover pretraining and fine-tuning, for the encoders and the overall neural network, respectively. \n",
    "Finally, [Step 4](#step-4-recommend) and [Step 5](#step-5-api-prediction) are dedicated to examples of candidate aptamer recommendation and API prediction, respectively.\n",
    "\n",
    "##### References\n",
    "<a id=\"ref-1\"></a>\n",
    "[1] Shin, Incheol, et al. \"AptaTrans: a deep neural network for predicting aptamer-protein interaction using pretrained encoders.\" BMC bioinformatics 24.1 (2023): 447. <br>\n",
    "<a id=\"ref-2\"></a>\n",
    "[2] Danaee, Padideh, et al. \"bpRNA: large-scale automated annotation and analysis of RNA secondary structure.\" Nucleic acids research 46.11 (2018): 5381-5394. <br>\n",
    "<a id=\"ref-3\"></a>\n",
    "[3] Berman, Helen M., et al. \"The protein data bank.\" Nucleic acids research 28.1 (2000): 235-242. <br>\n",
    "<a id=\"ref-4\"></a>\n",
    "[4] Li, Bi-Qing, et al. \"Prediction of aptamer-target interacting pairs with pseudo-amino acid composition.\" PLoS One 9.1 (2014): e86729. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data preparation\n",
    "Here, we prepare the data needed for pretraining and fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load (RNA) aptamer data for pretraining\n",
    "For pretraining the aptamer encoder, we use $79,890$ RNA aptamer sequences from the *bpRNA-1m* dataset from *bpRNA* [[2](#ref-2)].\n",
    "\n",
    "The sequences are augmented by adding their reverse complements. Then, they are masked to a numerical format suitable for the encoder and stored in PyTorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.) load the RNA dataset for pretraining\n",
    "apta_dataset = load_hf_dataset(name=\"bpRNA-shin2023\", store=True)\n",
    "\n",
    "# (2.) Create training-test splits of (sequence, secondary structure (ss)) pairs\n",
    "x_apta_train, x_apta_test, y_apta_train, y_apta_test = train_test_split(\n",
    "    apta_dataset[\"SEQUENCE\"].tolist(),\n",
    "    apta_dataset[\"SS\"].tolist(),\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RAMDOM_STATE,\n",
    ")\n",
    "\n",
    "# (3.) augment training data by adding reverse complements\n",
    "# e.g., (seq=\"ACG\", ss=\"SHM\") -> (seq=\"GCA\", ss=\"MHS\")\n",
    "x_apta_train, y_apta_train = augment_reverse(x_apta_train, y_apta_train)\n",
    "\n",
    "# (4.) Convert aptamer sequences and secondary structures to (integer) numerical vectors\n",
    "x_apta_train = rna2vec(x_apta_train, sequence_type=\"rna\")\n",
    "y_apta_train = rna2vec(y_apta_train, sequence_type=\"ss\")\n",
    "\n",
    "# (5.) mask the dataset for pretraining embeddings\n",
    "train_apta = MaskedDataset(\n",
    "    x=x_apta_train,\n",
    "    y=y_apta_train,\n",
    "    max_len=APTA_MAX_LEN,\n",
    "    mask_idx=N_APTA_VOCABS - 1,\n",
    "    is_rna=True,\n",
    ")\n",
    "test_apta = MaskedDataset(\n",
    "    x=x_apta_test,\n",
    "    y=y_apta_test,\n",
    "    max_len=APTA_MAX_LEN,\n",
    "    mask_idx=N_APTA_VOCABS - 1,\n",
    "    is_rna=True,\n",
    ")\n",
    "\n",
    "# (6.) create dataloaders\n",
    "train_apta_dataloader = DataLoader(\n",
    "    train_apta,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_apta_dataloader = DataLoader(\n",
    "    test_apta,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load protein data for pretraining\n",
    "For pretraining the protein encoder, we use $166,136$ protein sequences from the Protein Data Bank (PDB) [[3](#ref-3)].\n",
    "\n",
    "In this case, the sequences are not augmented by adding the reverse complements. However, protein words with below average frequency are filtered out. Then, similarly to above, sequences are transformed to a numerical representation suitable for the encoder and stored in PyTorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.) load the proteins' dataset for pretraining\n",
    "prot_dataset = load_hf_dataset(name=\"proteins-shin2023\", store=True)\n",
    "prot_words = load_csv_dataset(name=\"protein_word_freq\")  # words and their frequencies\n",
    "prot_words = prot_words.set_index(\"seq\")[\"freq\"].to_dict()\n",
    "\n",
    "# (2.) Create training-test splits of (sequence, secondary structure (ss)) pairs\n",
    "x_prot_train, x_prot_test, y_prot_train, y_prot_test = train_test_split(\n",
    "    prot_dataset[\"SEQUENCE\"].tolist(),\n",
    "    prot_dataset[\"SS\"].tolist(),\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RAMDOM_STATE,\n",
    ")\n",
    "\n",
    "# (3.) transform sequences to a numerical representation (vectors)\n",
    "filtered_prot_words = filter_words(prot_words)  # filter below average frequency words\n",
    "x_prot_train, y_prot_train = seq2vec(\n",
    "    sequence_list=(x_prot_train, y_prot_train),\n",
    "    words=filtered_prot_words,\n",
    "    seq_max_len=PROT_MAX_LEN,\n",
    ")\n",
    "x_prot_test, y_prot_test = seq2vec(\n",
    "    sequence_list=(x_prot_test, y_prot_test),\n",
    "    words=filtered_prot_words,\n",
    "    seq_max_len=PROT_MAX_LEN,\n",
    ")\n",
    "\n",
    "# (4.) mask the dataset for pretraining embeddings\n",
    "train_prot = MaskedDataset(\n",
    "    x=x_prot_train,\n",
    "    y=y_prot_train,\n",
    "    max_len=PROT_MAX_LEN,\n",
    "    mask_idx=N_PROT_VOCABS - 1,\n",
    "    is_rna=True,\n",
    ")\n",
    "test_prot = MaskedDataset(\n",
    "    x=x_prot_test,\n",
    "    y=y_prot_test,\n",
    "    max_len=PROT_MAX_LEN,\n",
    "    mask_idx=N_PROT_VOCABS - 1,\n",
    "    is_rna=False,\n",
    ")\n",
    "\n",
    "# (5.) create dataloaders\n",
    "train_prot_dataloader = DataLoader(\n",
    "    train_prot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_prot_dataloader = DataLoader(\n",
    "    test_prot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load aptamer-protein interaction (API) dataset\n",
    "For fine-tuning (i.e., train the neural network on the task of API prediction), we employ a selection of (aptamer, protein) pairs known to bind or not from [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.) load the api dataset for fine-tuning\n",
    "train_dataset = load_csv_dataset(name=\"train_li2014\")\n",
    "test_dataset = load_csv_dataset(name=\"test_li2014\")\n",
    "\n",
    "# (2.) create the API dataset\n",
    "train_dataset = APIDataset(\n",
    "    x_apta=train_dataset[\"aptamer\"].to_numpy(),\n",
    "    x_prot=train_dataset[\"protein\"].to_numpy(),\n",
    "    y=train_dataset[\"label\"].to_numpy(),\n",
    "    apta_max_len=APTA_MAX_LEN,\n",
    "    prot_max_len=PROT_MAX_LEN,\n",
    "    prot_words=filtered_prot_words,\n",
    ")\n",
    "test_dataset = APIDataset(\n",
    "    x_apta=test_dataset[\"aptamer\"].to_numpy(),\n",
    "    x_prot=test_dataset[\"protein\"].to_numpy(),\n",
    "    y=test_dataset[\"label\"].to_numpy(),\n",
    "    apta_max_len=APTA_MAX_LEN,\n",
    "    prot_max_len=PROT_MAX_LEN,\n",
    "    prot_words=filtered_prot_words,\n",
    "    split=\"test\",\n",
    ")\n",
    "\n",
    "# (3.) create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model\n",
    "Here, we initialize AptaTrans model, employing the same hyperparameters from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.) define embedding configurations for the encoders\n",
    "apta_embedding = EncoderPredictorConfig(\n",
    "    num_embeddings=N_APTA_VOCABS,\n",
    "    target_dim=N_APTA_TARGET_VOCABS,\n",
    "    max_len=APTA_MAX_LEN,\n",
    ")\n",
    "prot_embedding = EncoderPredictorConfig(\n",
    "    num_embeddings=N_PROT_VOCABS,\n",
    "    target_dim=N_PROT_TARGET_VOCABS,\n",
    "    max_len=PROT_MAX_LEN,\n",
    ")\n",
    "\n",
    "# (2.) initialize the model\n",
    "model = AptaTrans(\n",
    "    apta_embedding=apta_embedding,\n",
    "    prot_embedding=prot_embedding,\n",
    "    in_dim=128,\n",
    "    n_encoder_layers=6,\n",
    "    n_heads=8,\n",
    "    conv_layers=[3, 3, 3],\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "# (optional) (2.1) load pretrained weights\n",
    "model.load_pretrained_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training\n",
    "For training, we leverage the `Lightning` framework. Hyperparameters related to optimization are already set to the ones used in the paper.\n",
    "\n",
    "If you decide to load pretrained weights, you may skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining\n",
    "Sequential pre-training of the aptamer and protein encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_lightning_apta = AptaTransEncoderLightning(\n",
    "    model=model,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    encoder_type=\"apta\",\n",
    ").to(device)\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(encoder_lightning_apta, train_apta_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_prot_lightning = AptaTransEncoderLightning(\n",
    "    model=model,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    encoder_type=\"prot\",\n",
    ").to(device)\n",
    "trainer = L.Trainer(max_epochs=1000)\n",
    "trainer.fit(encoder_prot_lightning, train_prot_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Fine-tuning the deep neural network by training for the task of API prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lightning = AptaTransLightning(\n",
    "    model=model,\n",
    "    lr=1e-5,\n",
    "    weight_decay=1e-5,\n",
    ").to(device)\n",
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(model_lightning, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Recommend\n",
    "Here, you may generate candidate aptamers for a given target protein sequence.\n",
    "\n",
    "In `AptaTransPipeline` we combine Monte Carlo Tree Search (MCTS) and AptaTrans' deep neural network for generating candidates and ranking them, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the target protein sequence here\n",
    "target_protein = (\n",
    "    \"STEYKLVVVGADGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLLDILDTAGQEEYSAM\"\n",
    "    \"RDQYMRTGEGFLCVFAINNTKSFEDIHHYREQIKRVKDSEDVPMVLVGNKCDLPSRTVDTKQAQDLARSYGIPFIETSAKTR\"\n",
    "    \"QGVDDAFYTLVREIRKHKEKMSK\"\n",
    ")\n",
    "\n",
    "pipeline = AptaTransPipeline(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    prot_words=prot_words,\n",
    "    depth=20,  # depth of the search, length of generated candidates\n",
    "    n_iterations=10,  # number of iterations (higher is better, but slower)\n",
    ")\n",
    "candidates = pipeline.recommend(\n",
    "    target=target_protein,\n",
    "    n_candidates=10,  # number of candidates to generate\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print generate candidates and their score (higher is better)\n",
    "for idx, candidate in enumerate(candidates):\n",
    "    print(f\"[Candidate {idx + 1}] {candidate[0]} - Score: {float(candidate[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: API prediction\n",
    "Here, you may predict whether a given aptamer and protein sequence bind or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the target protein sequence here\n",
    "target_protein = (\n",
    "    \"STEYKLVVVGADGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLLDILDTAGQEEYSAM\"\n",
    "    \"RDQYMRTGEGFLCVFAINNTKSFEDIHHYREQIKRVKDSEDVPMVLVGNKCDLPSRTVDTKQAQDLARSYGIPFIETSAKTR\"\n",
    "    \"QGVDDAFYTLVREIRKHKEKMSK\"\n",
    ")\n",
    "aptamer_candidate = \"...\"  # specify the aptamer candidate sequence here\n",
    "\n",
    "experiment = Aptamer(\n",
    "    target=target_protein, model=model, device=device, prot_words=filtered_prot_words\n",
    ")\n",
    "score = experiment.evaluate(aptamer_candidate=aptamer_candidate)\n",
    "print(f\"Score: {score.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyaptamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
