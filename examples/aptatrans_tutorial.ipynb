{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pyaptamer.aptatrans import (\n",
    "    AptaTrans,\n",
    "    AptaTransPipeline,\n",
    "    AptaTransSolver,\n",
    "    EncoderPredictorConfig,\n",
    ")\n",
    "from pyaptamer.datasets import (\n",
    "    load_csv_dataset,\n",
    "    load_hf_dataset,\n",
    ")\n",
    "from pyaptamer.datasets.dataclasses import MaskedDataset\n",
    "from pyaptamer.utils._augment import augment_reverse\n",
    "from pyaptamer.utils._base import (\n",
    "    filter_words,\n",
    "    seq2vec,\n",
    ")\n",
    "\n",
    "# auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_SIZE = 0.05\n",
    "RAMDOM_STATE = 42  # for reproducibility\n",
    "\n",
    "# embedding configuras for pretraining\n",
    "# aptamers\n",
    "N_APTA_VOCABS = 127\n",
    "N_APTA_TARGET_VOCABS = 344\n",
    "APTA_MAX_LEN = 275\n",
    "# proteins\n",
    "N_PROT_VOCABS = 715\n",
    "N_PROT_TARGET_VOCABS = 585\n",
    "PROT_MAX_LEN = 867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RNA data for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.) load the RNA dataset for pretraining\n",
    "rna_dataset = load_hf_dataset(name=\"bpRNA-shin2023\", store=True)\n",
    "\n",
    "# (2.) Creaye training-test splits of (sequence, secondary structure (ss)) pairs\n",
    "x_rna_train, x_rna_test, y_rna_train, y_rna_test = train_test_split(\n",
    "    rna_dataset[\"SEQUENCE\"].tolist(),\n",
    "    rna_dataset[\"SS\"].tolist(),\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RAMDOM_STATE,\n",
    ")\n",
    "\n",
    "# (3.) augment training data by adding reverse complements\n",
    "# e.g., (seq=\"ACG\", ss=\"SHM\") -> (seq=\"GCA\", ss=\"MHS\")\n",
    "x_rna_train, y_rna_train = augment_reverse(x_rna_train, y_rna_train)\n",
    "\n",
    "# (4.) mask the dataset for pretraining embeddings\n",
    "train_rna = MaskedDataset(\n",
    "    x=x_rna_train,\n",
    "    y=y_rna_train,\n",
    "    max_len=APTA_MAX_LEN,\n",
    "    mask_idx=N_APTA_VOCABS - 1,\n",
    "    is_rna=True,\n",
    ")\n",
    "test_rna = MaskedDataset(\n",
    "    x=x_rna_test,\n",
    "    y=y_rna_test,\n",
    "    max_len=APTA_MAX_LEN,\n",
    "    mask_idx=N_APTA_VOCABS - 1,\n",
    "    is_rna=True,\n",
    ")\n",
    "\n",
    "# (5.) create dataloaders\n",
    "train_rna_dataloader = DataLoader(\n",
    "    train_rna,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_rna_dataloader = DataLoader(\n",
    "    test_rna,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load protein data for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.) load the proteins' dataset for pretraining\n",
    "prot_dataset = load_hf_dataset(name=\"proteins-shin2023\", store=True)\n",
    "prot_words = load_csv_dataset(name=\"protein_word_freq\")  # words and their frequencies\n",
    "prot_words = prot_words.set_index(\"seq\")[\"freq\"].to_dict()\n",
    "\n",
    "# (2.) Creaye training-test splits of (sequence, secondary structure (ss)) pairs\n",
    "x_prot_train, x_prot_test, y_prot_train, y_prot_test = train_test_split(\n",
    "    prot_dataset[\"SEQUENCE\"].tolist(),\n",
    "    prot_dataset[\"SS\"].tolist(),\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RAMDOM_STATE,\n",
    ")\n",
    "\n",
    "# (3.) transform sequences to a numerical representation (vectors)\n",
    "filtered_prot_words = filter_words(prot_words)\n",
    "x_prot_train, y_prot_train = seq2vec(\n",
    "    sequence_list=(x_prot_train, y_prot_train),\n",
    "    words=filtered_prot_words,\n",
    "    seq_max_len=PROT_MAX_LEN,\n",
    ")\n",
    "x_prot_test, y_prot_test = seq2vec(\n",
    "    sequence_list=(x_prot_test, y_prot_test),\n",
    "    words=filtered_prot_words,\n",
    "    seq_max_len=PROT_MAX_LEN,\n",
    ")\n",
    "\n",
    "# (4.) mask the dataset for pretraining embeddings\n",
    "train_prot = MaskedDataset(\n",
    "    x=x_prot_train,\n",
    "    y=y_prot_train,\n",
    "    max_len=PROT_MAX_LEN,\n",
    "    mask_idx=N_PROT_VOCABS - 1,\n",
    "    is_rna=True,\n",
    ")\n",
    "test_prot = MaskedDataset(\n",
    "    x=x_prot_test,\n",
    "    y=y_prot_test,\n",
    "    max_len=PROT_MAX_LEN,\n",
    "    mask_idx=N_PROT_VOCABS - 1,\n",
    "    is_rna=False,\n",
    ")\n",
    "\n",
    "# (5.) create dataloaders\n",
    "train_prot_dataloader = DataLoader(\n",
    "    train_prot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_prot_dataloader = DataLoader(\n",
    "    test_prot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load aptamer-protein interaction (API) dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_csv_dataset(name=\"train_li2014\")\n",
    "test_dataset = load_csv_dataset(name=\"test_li2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apta_embedding = EncoderPredictorConfig(\n",
    "    num_embeddings=N_APTA_VOCABS,\n",
    "    target_dim=N_APTA_TARGET_VOCABS,\n",
    "    max_len=APTA_MAX_LEN,\n",
    ")\n",
    "prot_embedding = EncoderPredictorConfig(\n",
    "    num_embeddings=N_PROT_VOCABS,\n",
    "    target_dim=N_PROT_TARGET_VOCABS,\n",
    "    max_len=PROT_MAX_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AptaTrans(\n",
    "    apta_embedding=apta_embedding,\n",
    "    prot_embedding=prot_embedding,\n",
    "    in_dim=128,\n",
    "    n_encoder_layers=6,\n",
    "    n_heads=8,\n",
    "    conv_layers=[3, 3, 3],\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = AptaTransSolver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AptaTransPipeline(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    prot_words=prot_words,\n",
    "    # depth=40, # i.e., how long the candidates will be\n",
    "    # n_iterations=1000,\n",
    "    depth=5,  # this determine the length of the candidates\n",
    "    n_iterations=1,\n",
    ")\n",
    "\n",
    "# example: human insulin - A chain\n",
    "target_protein = \"GIVEQCCTSICSLYQLENYCN\"  # specify the target protein sequence here\n",
    "candidates = pipeline.recommend(\n",
    "    target=target_protein,\n",
    "    n_candidates=10,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyaptamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
