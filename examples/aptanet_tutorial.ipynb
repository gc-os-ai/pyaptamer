{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e8d179",
   "metadata": {},
   "source": [
    "# Binding prediction using AptaNet\n",
    "Step-by-step guide to using AptaNet for binary aptamer–protein binding prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a64683",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- **pairs_to_features**: converts `(aptamer_seq, protein_seq)` pairs into feature vectors using k-mer + PSeAAC.\n",
    "- **FeatureSelector**: a Random Forest-based transformer that selects important features.\n",
    "- **SkorchAptaNet**: a PyTorch MLP wrapped in Skorch for binary classification with a configurable threshold.\n",
    "- **load_pfoa_structure**: helper to load PFOA molecule structure from PDB file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a5052",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import the core functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15eabec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torch  # noqa: I001\n",
    "import torch.optim as optim\n",
    "\n",
    "# Data imports\n",
    "from pyaptamer.datasets.loader import load_1gnh_structure\n",
    "from pyaptamer.utils.struct_to_aaseq import struct_to_aaseq\n",
    "\n",
    "# If you want to use the aptamer pipeline, you should use the following imports\n",
    "\n",
    "# If you to build your own aptamer pipeline, you should use the following imports\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pyaptamer.aptanet import FeatureSelector, SkorchAptaNet\n",
    "from pyaptamer.utils._aptanet_utils import pairs_to_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c592d9",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "To train the skorch network the notebook uses:\n",
    "* As `X`:\n",
    "    * 5 random aptamer sequences of length>30 (to satisfy the default lambda value of 30 set in the PSeAAC algorithm).\n",
    "    * Amino-acid sequences extracted from the 1GNH protein molecule.\n",
    "* As `y`:\n",
    " * A random binary value (0/1) equal to the number of `(aptamer_seq, protein_seq)` pairs as dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f6701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_sequences = [\n",
    "    \"GGGAGGACGAAGACGACUCGAGACAGGCUAGGGAGGGA\",\n",
    "    \"AAGCGUCGGAUCUACACGUGCGAUAGCUCAGUACGCGGU\",\n",
    "    \"CGGUAUCGAGUACAGGAGUCCGACGGAUAGUCCGGAGC\",\n",
    "    \"UAGCUAGCGAACUAGGCGUAGCUUCGAGUAGCUACGGAA\",\n",
    "    \"GCUAGGACGAUCGCACGUGACCGUCAGUAGCGUAGGAGA\",\n",
    "]\n",
    "\n",
    "gnh = load_1gnh_structure()\n",
    "protein_sequence = struct_to_aaseq(gnh)\n",
    "\n",
    "unique_proteins = list(set(protein_sequence))\n",
    "unique_aptamers = list(set(aptamer_sequences))\n",
    "\n",
    "# Build all combinations (aptamer, protein)\n",
    "X = [(a, p) for a in unique_aptamers for p in unique_proteins]\n",
    "\n",
    "# Dummy binary labels for the pairs\n",
    "y = torch.randint(0, 2, (len(X),), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd459190",
   "metadata": {},
   "source": [
    "## Build your own pipeline\n",
    " To build a scikit-learn pipeline, follow these steps:\n",
    "1. Convert the input to the desired (aptamer_sequence, protein_sequence) format.\n",
    "    * OPTIONAL: As mentioned in the paper, perform under-sampling using the  \n",
    "    Neighborhood Cleaning Rule to balance the classes.\n",
    "2. Get the PSeAAC feature vectors for your converted input (using `pairs_to_features`).\n",
    "3. Select the number of features to use from the feature vector (using `FeatureSelector`).\n",
    "4. Define the skorch neural network (using `SkorchAptaNet`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cf0bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: If you want to use the Neighborhood Cleaning Rule for under-sampling\n",
    "# %pip install imblearn\n",
    "# from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "# ncr = NeighbourhoodCleaningRule()\n",
    "# X, y = ncr.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13fb1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "net = SkorchAptaNet(\n",
    "    module__hidden_dim=128,\n",
    "    module__n_hidden=7,\n",
    "    module__dropout=0.3,\n",
    "    max_epochs=200,\n",
    "    lr=1.4e-4,\n",
    "    batch_size=310,\n",
    "    optimizer=optim.RMSprop,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    threshold=0.5,\n",
    "    train_split=None,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "feature_transformer = FunctionTransformer(\n",
    "    func=pairs_to_features,\n",
    "    validate=False,\n",
    "    # Optional arguments for pairs_to_features\n",
    "    # example: kw_args={'k': 4, 'pseaac_kwargs': {'lambda_value': 30}}\n",
    "    kw_args={},\n",
    ")\n",
    "\n",
    "# Option 1: build a new pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"features\", feature_transformer),\n",
    "        (\"selector\", FeatureSelector()),\n",
    "        (\"clf\", net),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Option 2: import the pre-defined pipeline (which does the same)\n",
    "# pipeline = pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70138a",
   "metadata": {},
   "source": [
    "## Model Training and Prediction\n",
    "\n",
    "Now that we’ve defined our AptaNet pipeline, we proceed to train the model, and use it to predict, on our aptamer-protein dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ed76399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.8461\u001b[0m  0.1419\n",
      "      2        \u001b[36m0.6572\u001b[0m  0.0161\n",
      "      3        0.7241  0.0000\n",
      "      4        0.6856  0.0183\n",
      "      5        \u001b[36m0.5405\u001b[0m  0.0114\n",
      "      6        0.9785  0.0000\n",
      "      7        0.8498  0.0159\n",
      "      8        0.8247  0.0000\n",
      "      9        0.7223  0.0157\n",
      "     10        0.6711  0.0000\n",
      "     11        0.6652  0.0158\n",
      "     12        0.6684  0.0000\n",
      "     13        0.7197  0.0158\n",
      "     14        0.6808  0.0000\n",
      "     15        0.5502  0.0158\n",
      "     16        0.7807  0.0000\n",
      "     17        0.6591  0.0159\n",
      "     18        0.6840  0.0157\n",
      "     19        0.6722  0.0000\n",
      "     20        0.5827  0.0159\n",
      "     21        0.8277  0.0090\n",
      "     22        0.6763  0.0000\n",
      "     23        0.7158  0.0162\n",
      "     24        0.7339  0.0000\n",
      "     25        0.7955  0.0152\n",
      "     26        0.8719  0.0000\n",
      "     27        0.6885  0.0157\n",
      "     28        0.6226  0.0000\n",
      "     29        0.6545  0.0150\n",
      "     30        0.6366  0.0000\n",
      "     31        0.6825  0.0158\n",
      "     32        0.6082  0.0000\n",
      "     33        0.7029  0.0165\n",
      "     34        0.7299  0.0154\n",
      "     35        0.9808  0.0000\n",
      "     36        0.7594  0.0158\n",
      "     37        0.8403  0.0039\n",
      "     38        0.7218  0.0000\n",
      "     39        0.7866  0.0161\n",
      "     40        0.6794  0.0000\n",
      "     41        0.7263  0.0160\n",
      "     42        0.6516  0.0000\n",
      "     43        0.6717  0.0158\n",
      "     44        0.8348  0.0160\n",
      "     45        0.7508  0.0000\n",
      "     46        0.9021  0.0000\n",
      "     47        0.7072  0.0150\n",
      "     48        0.7792  0.0000\n",
      "     49        0.7591  0.0159\n",
      "     50        0.7266  0.0000\n",
      "     51        0.6756  0.0157\n",
      "     52        0.6163  0.0000\n",
      "     53        0.6225  0.0160\n",
      "     54        0.6639  0.0000\n",
      "     55        0.7083  0.0159\n",
      "     56        0.5963  0.0000\n",
      "     57        0.7257  0.0157\n",
      "     58        0.6405  0.0056\n",
      "     59        0.7366  0.0102\n",
      "     60        0.6195  0.0000\n",
      "     61        0.7516  0.0158\n",
      "     62        0.6718  0.0000\n",
      "     63        0.5932  0.0159\n",
      "     64        0.8882  0.0000\n",
      "     65        0.7327  0.0160\n",
      "     66        0.6991  0.0000\n",
      "     67        0.6891  0.0157\n",
      "     68        0.5933  0.0000\n",
      "     69        0.8549  0.0111\n",
      "     70        0.8108  0.0000\n",
      "     71        \u001b[36m0.5103\u001b[0m  0.0153\n",
      "     72        0.6099  0.0000\n",
      "     73        0.7089  0.0157\n",
      "     74        0.7832  0.0000\n",
      "     75        0.8883  0.0000\n",
      "     76        0.6810  0.0189\n",
      "     77        0.8062  0.0000\n",
      "     78        0.7043  0.0000\n",
      "     79        0.7434  0.0159\n",
      "     80        0.5981  0.0000\n",
      "     81        0.8158  0.0176\n",
      "     82        0.6205  0.0000\n",
      "     83        0.6938  0.0000\n",
      "     84        0.5907  0.0158\n",
      "     85        0.7626  0.0000\n",
      "     86        0.8219  0.0163\n",
      "     87        0.8149  0.0000\n",
      "     88        0.6653  0.0152\n",
      "     89        0.9269  0.0000\n",
      "     90        0.6597  0.0000\n",
      "     91        0.6851  0.0160\n",
      "     92        0.8428  0.0000\n",
      "     93        0.8266  0.0160\n",
      "     94        0.8843  0.0000\n",
      "     95        0.7539  0.0000\n",
      "     96        0.8180  0.0158\n",
      "     97        0.8543  0.0000\n",
      "     98        0.6094  0.0159\n",
      "     99        0.7100  0.0000\n",
      "    100        0.8850  0.0000\n",
      "    101        0.7484  0.0158\n",
      "    102        0.7033  0.0000\n",
      "    103        0.6150  0.0101\n",
      "    104        0.8367  0.0000\n",
      "    105        0.5537  0.0000\n",
      "    106        0.7578  0.0000\n",
      "    107        0.7593  0.0000\n",
      "    108        0.8357  0.0158\n",
      "    109        0.8099  0.0088\n",
      "    110        0.7393  0.0000\n",
      "    111        0.6879  0.0000\n",
      "    112        0.6998  0.0158\n",
      "    113        0.6404  0.0000\n",
      "    114        0.6996  0.0159\n",
      "    115        0.8520  0.0000\n",
      "    116        0.6182  0.0000\n",
      "    117        0.6585  0.0179\n",
      "    118        0.7316  0.0000\n",
      "    119        1.1209  0.0139\n",
      "    120        0.8521  0.0000\n",
      "    121        0.7580  0.0000\n",
      "    122        0.7664  0.0000\n",
      "    123        0.6473  0.0000\n",
      "    124        0.8394  0.0157\n",
      "    125        0.6356  0.0070\n",
      "    126        0.8493  0.0106\n",
      "    127        0.8244  0.0120\n",
      "    128        0.6774  0.0099\n",
      "    129        0.6474  0.0085\n",
      "    130        0.7050  0.0078\n",
      "    131        0.7881  0.0136\n",
      "    132        0.5723  0.0080\n",
      "    133        0.7643  0.0076\n",
      "    134        0.5388  0.0098\n",
      "    135        0.7935  0.0080\n",
      "    136        0.8746  0.0080\n",
      "    137        0.7145  0.0071\n",
      "    138        0.7351  0.0096\n",
      "    139        0.6367  0.0103\n",
      "    140        0.9759  0.0052\n",
      "    141        0.5974  0.0126\n",
      "    142        0.5716  0.0069\n",
      "    143        0.7519  0.0074\n",
      "    144        0.7116  0.0011\n",
      "    145        0.6647  0.0123\n",
      "    146        0.7062  0.0032\n",
      "    147        0.7479  0.0000\n",
      "    148        0.6261  0.0150\n",
      "    149        0.6971  0.0000\n",
      "    150        0.7539  0.0166\n",
      "    151        0.8594  0.0000\n",
      "    152        0.6066  0.0151\n",
      "    153        0.8738  0.0016\n",
      "    154        0.7141  0.0098\n",
      "    155        0.7691  0.0052\n",
      "    156        0.7982  0.0094\n",
      "    157        0.6404  0.0071\n",
      "    158        0.8334  0.0000\n",
      "    159        0.7291  0.0151\n",
      "    160        0.6554  0.0005\n",
      "    161        0.6280  0.0097\n",
      "    162        0.6236  0.0077\n",
      "    163        0.5305  0.0000\n",
      "    164        0.6411  0.0157\n",
      "    165        0.6683  0.0000\n",
      "    166        0.6695  0.0104\n",
      "    167        0.7942  0.0087\n",
      "    168        \u001b[36m0.4477\u001b[0m  0.0000\n",
      "    169        0.7733  0.0184\n",
      "    170        0.7497  0.0042\n",
      "    171        0.6720  0.0098\n",
      "    172        0.6226  0.0060\n",
      "    173        0.5872  0.0061\n",
      "    174        0.6602  0.0062\n",
      "    175        0.7334  0.0060\n",
      "    176        0.7552  0.0074\n",
      "    177        0.6609  0.0000\n",
      "    178        0.8628  0.0138\n",
      "    179        0.6421  0.0000\n",
      "    180        0.6228  0.0093\n",
      "    181        0.6133  0.0069\n",
      "    182        0.7955  0.0098\n",
      "    183        0.6620  0.0083\n",
      "    184        0.8551  0.0000\n",
      "    185        0.6537  0.0054\n",
      "    186        0.7264  0.0000\n",
      "    187        0.7663  0.0098\n",
      "    188        0.7279  0.0010\n",
      "    189        0.7689  0.0104\n",
      "    190        0.7475  0.0081\n",
      "    191        0.7611  0.0000\n",
      "    192        0.7346  0.0138\n",
      "    193        0.8142  0.0006\n",
      "    194        0.7191  0.0095\n",
      "    195        0.8565  0.0096\n",
      "    196        0.6862  0.0008\n",
      "    197        0.6632  0.0129\n",
      "    198        0.8455  0.0055\n",
      "    199        0.7827  0.0000\n",
      "    200        0.8276  0.0149\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline on the aptamer-protein pairs\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Predict the labels for the training data\n",
    "y_pred = pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945dc876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Optional: Evaluate training accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_score(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyaptamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
