{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e8d179",
   "metadata": {},
   "source": [
    "# Binding prediction using AptaNet\n",
    "Step-by-step guide to using AptaNet for binary aptamer–protein binding prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a64683",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- **pairs_to_features**: converts `(aptamer_seq, protein_seq)` pairs into feature vectors using k-mer + PSeAAC.\n",
    "- **FeatureSelector**: a Random Forest-based transformer that selects important features.\n",
    "- **SkorchAptaNet**: a PyTorch MLP wrapped in Skorch for binary classification with a configurable threshold.\n",
    "- **load_pfoa_structure**: helper to load PFOA molecule structure from PDB file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a5052",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import the core functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eabec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torch  # noqa: I001\n",
    "\n",
    "# Data imports\n",
    "from pyaptamer.datasets import load_1gnh_structure\n",
    "from pyaptamer.utils.struct_to_aaseq import struct_to_aaseq\n",
    "\n",
    "# If you to build your own aptamer pipeline, you should use the following imports\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from pyaptamer.utils._aptanet_utils import pairs_to_features\n",
    "from pyaptamer.aptanet._aptanet_nn import AptaNetMLP\n",
    "\n",
    "# If you want to use the AptaNet pipeline, you can import it directly\n",
    "from pyaptamer.aptanet import AptaNetPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c592d9",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "To train the skorch network the notebook uses:\n",
    "* As `X`:\n",
    "    * 5 random aptamer sequences of length>30 (to satisfy the default lambda value of 30 set in the PSeAAC algorithm).\n",
    "    * Amino-acid sequences extracted from the 1GNH protein molecule.\n",
    "* As `y`:\n",
    " * A random binary value (0/1) equal to the number of `(aptamer_seq, protein_seq)` pairs as dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_sequence = [\n",
    "    \"GGGAGGACGAAGACGACUCGAGACAGGCUAGGGAGGGA\",\n",
    "    \"AAGCGUCGGAUCUACACGUGCGAUAGCUCAGUACGCGGU\",\n",
    "    \"CGGUAUCGAGUACAGGAGUCCGACGGAUAGUCCGGAGC\",\n",
    "    \"UAGCUAGCGAACUAGGCGUAGCUUCGAGUAGCUACGGAA\",\n",
    "    \"GCUAGGACGAUCGCACGUGACCGUCAGUAGCGUAGGAGA\",\n",
    "]\n",
    "\n",
    "gnh = load_1gnh_structure()\n",
    "protein_sequence = struct_to_aaseq(gnh)\n",
    "\n",
    "# Build all combinations (aptamer, protein)\n",
    "X = [(a, p) for a in aptamer_sequence for p in protein_sequence]\n",
    "\n",
    "# Dummy binary labels for the pairs\n",
    "y = torch.randint(0, 2, (len(X),), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd459190",
   "metadata": {},
   "source": [
    "## Build your own pipeline\n",
    " To build a scikit-learn pipeline, follow these steps:\n",
    "1. Convert the input to the desired (aptamer_sequence, protein_sequence) format.\n",
    "    * OPTIONAL: As mentioned in the paper, perform under-sampling using the  \n",
    "    Neighborhood Cleaning Rule to balance the classes.\n",
    "2. Get the PSeAAC feature vectors for your converted input (using `pairs_to_features`).\n",
    "3. Select the number of features to use from the feature vector (using `FeatureSelector`).\n",
    "4. Define the skorch neural network (using `SkorchAptaNet`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0cf0bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: If you want to use the Neighborhood Cleaning Rule for under-sampling\n",
    "# %pip install imblearn\n",
    "# from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "# ncr = NeighbourhoodCleaningRule()\n",
    "# X, y = ncr.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13fb1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformer = FunctionTransformer(\n",
    "    func=pairs_to_features,\n",
    "    validate=False,\n",
    "    # Optional arguments for pairs_to_features\n",
    "    # example: kw_args={'k': 4, 'pseaac_kwargs': {'lambda_value': 30}}\n",
    "    kw_args={},\n",
    ")\n",
    "\n",
    "selector = SelectFromModel(\n",
    "    estimator=RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=9,\n",
    "        random_state=None,\n",
    "    ),\n",
    "    threshold=\"mean\",\n",
    ")\n",
    "\n",
    "# Define the classifier\n",
    "net = NeuralNetBinaryClassifier(\n",
    "    module=AptaNetMLP,\n",
    "    module__input_dim=None,\n",
    "    module__hidden_dim=128,\n",
    "    module__n_hidden=7,\n",
    "    module__dropout=0.3,\n",
    "    module__output_dim=1,\n",
    "    module__use_lazy=True,\n",
    "    criterion=torch.nn.BCEWithLogitsLoss,\n",
    "    max_epochs=200,\n",
    "    lr=0.00014,\n",
    "    optimizer=torch.optim.RMSprop,\n",
    "    optimizer__alpha=0.9,\n",
    "    optimizer__eps=1e-08,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Option 1: build a new pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"features\", feature_transformer),\n",
    "        (\"selector\", selector),\n",
    "        (\"clf\", net),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Option 2: import the pre-defined pipeline (which does the same)\n",
    "pipeline = AptaNetPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70138a",
   "metadata": {},
   "source": [
    "## Model Training and Prediction\n",
    "\n",
    "Now that we’ve defined our AptaNet pipeline, we proceed to train the model, and use it to predict, on our aptamer-protein dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ed76399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7777\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.6932\u001b[0m  0.0107\n",
      "      2        \u001b[36m0.7505\u001b[0m       0.5000        \u001b[35m0.6932\u001b[0m  0.0082\n",
      "      3        \u001b[36m0.7306\u001b[0m       0.5000        \u001b[35m0.6932\u001b[0m  0.0098\n",
      "      4        0.7702       0.5000        \u001b[35m0.6932\u001b[0m  0.0117\n",
      "      5        0.7840       0.5000        \u001b[35m0.6932\u001b[0m  0.0103\n",
      "      6        \u001b[36m0.6614\u001b[0m       0.5000        \u001b[35m0.6932\u001b[0m  0.0077\n",
      "      7        0.7181       0.5000        \u001b[35m0.6932\u001b[0m  0.0090\n",
      "      8        0.7265       0.5000        \u001b[35m0.6932\u001b[0m  0.0123\n",
      "      9        0.6886       0.5000        \u001b[35m0.6932\u001b[0m  0.0100\n",
      "     10        0.7455       0.5000        0.6932  0.0086\n",
      "     11        0.6847       0.5000        \u001b[35m0.6932\u001b[0m  0.0085\n",
      "     12        0.7366       0.5000        0.6932  0.0118\n",
      "     13        0.6705       0.5000        \u001b[35m0.6932\u001b[0m  0.0112\n",
      "     14        0.7077       0.5000        \u001b[35m0.6932\u001b[0m  0.0065\n",
      "     15        0.7336       0.5000        \u001b[35m0.6932\u001b[0m  0.0097\n",
      "     16        0.7134       0.5000        \u001b[35m0.6932\u001b[0m  0.0111\n",
      "     17        0.7668       0.5000        \u001b[35m0.6932\u001b[0m  0.0100\n",
      "     18        0.7296       0.5000        \u001b[35m0.6932\u001b[0m  0.0102\n",
      "     19        \u001b[36m0.6489\u001b[0m       0.5000        \u001b[35m0.6932\u001b[0m  0.0080\n",
      "     20        0.7638       0.5000        \u001b[35m0.6932\u001b[0m  0.0121\n",
      "     21        0.7005       0.5000        \u001b[35m0.6932\u001b[0m  0.0110\n",
      "     22        0.7522       0.5000        \u001b[35m0.6932\u001b[0m  0.0135\n",
      "     23        0.7275       0.5000        \u001b[35m0.6932\u001b[0m  0.0126\n",
      "     24        0.7554       0.5000        \u001b[35m0.6932\u001b[0m  0.0095\n",
      "     25        0.7553       0.5000        \u001b[35m0.6932\u001b[0m  0.0082\n",
      "     26        0.8077       0.5000        \u001b[35m0.6932\u001b[0m  0.0110\n",
      "     27        0.7231       0.5000        \u001b[35m0.6932\u001b[0m  0.0120\n",
      "     28        0.8158       0.5000        \u001b[35m0.6932\u001b[0m  0.0098\n",
      "     29        0.7765       0.5000        \u001b[35m0.6932\u001b[0m  0.0082\n",
      "     30        0.7091       0.5000        \u001b[35m0.6932\u001b[0m  0.0107\n",
      "     31        0.7092       0.5000        \u001b[35m0.6932\u001b[0m  0.0106\n",
      "     32        0.7015       0.5000        \u001b[35m0.6932\u001b[0m  0.0075\n",
      "     33        0.7029       0.5000        \u001b[35m0.6932\u001b[0m  0.0093\n",
      "     34        0.7200       0.5000        \u001b[35m0.6932\u001b[0m  0.0091\n",
      "     35        0.7578       0.5000        \u001b[35m0.6932\u001b[0m  0.0097\n",
      "     36        0.7247       0.5000        \u001b[35m0.6932\u001b[0m  0.0080\n",
      "     37        0.7285       0.5000        0.6932  0.0137\n",
      "     38        0.7258       0.5000        0.6932  0.0124\n",
      "     39        0.7568       0.5000        0.6932  0.0145\n",
      "     40        0.6936       0.5000        0.6932  0.0105\n",
      "     41        0.7253       0.5000        \u001b[35m0.6932\u001b[0m  0.0084\n",
      "     42        0.7225       0.5000        \u001b[35m0.6932\u001b[0m  0.0066\n",
      "     43        0.7297       0.5000        \u001b[35m0.6932\u001b[0m  0.0117\n",
      "     44        0.7041       0.5000        \u001b[35m0.6932\u001b[0m  0.0102\n",
      "     45        0.7410       0.5000        0.6932  0.0082\n",
      "     46        0.6927       0.5000        \u001b[35m0.6932\u001b[0m  0.0117\n",
      "     47        0.7425       0.5000        0.6932  0.0107\n",
      "     48        0.6742       0.5000        0.6932  0.0084\n",
      "     49        0.7108       0.5000        0.6932  0.0095\n",
      "     50        0.7785       0.5000        0.6932  0.0091\n",
      "     51        0.7541       0.5000        0.6932  0.0072\n",
      "     52        0.7910       0.5000        \u001b[35m0.6932\u001b[0m  0.0121\n",
      "     53        0.7094       0.5000        \u001b[35m0.6932\u001b[0m  0.0111\n",
      "     54        0.7590       0.5000        \u001b[35m0.6932\u001b[0m  0.0094\n",
      "     55        0.6671       0.5000        \u001b[35m0.6932\u001b[0m  0.0105\n",
      "     56        0.6936       0.5000        \u001b[35m0.6932\u001b[0m  0.0107\n",
      "     57        0.7444       0.5000        \u001b[35m0.6932\u001b[0m  0.0100\n",
      "     58        0.8350       0.5000        \u001b[35m0.6932\u001b[0m  0.0117\n",
      "     59        0.7264       0.5000        0.6932  0.0079\n",
      "     60        0.7017       0.5000        0.6932  0.0134\n",
      "     61        0.7242       0.5000        0.6932  0.0120\n",
      "     62        0.8309       0.5000        \u001b[35m0.6932\u001b[0m  0.0125\n",
      "     63        0.6884       0.5000        \u001b[35m0.6932\u001b[0m  0.0131\n",
      "     64        0.7428       0.5000        \u001b[35m0.6932\u001b[0m  0.0067\n",
      "     65        0.6931       0.5000        \u001b[35m0.6932\u001b[0m  0.0085\n",
      "     66        0.7033       0.5000        \u001b[35m0.6932\u001b[0m  0.0100\n",
      "     67        0.7066       0.5000        \u001b[35m0.6932\u001b[0m  0.0080\n",
      "     68        0.7317       0.5000        \u001b[35m0.6932\u001b[0m  0.0090\n",
      "     69        0.6902       0.5000        \u001b[35m0.6932\u001b[0m  0.0085\n",
      "     70        0.7191       0.5000        \u001b[35m0.6931\u001b[0m  0.0110\n",
      "     71        0.7122       0.5000        0.6931  0.0082\n",
      "     72        0.7619       0.5000        0.6931  0.0115\n",
      "     73        0.7577       0.5000        0.6931  0.0092\n",
      "     74        0.7609       0.5000        \u001b[35m0.6931\u001b[0m  0.0075\n",
      "     75        \u001b[36m0.6298\u001b[0m       0.5000        \u001b[35m0.6931\u001b[0m  0.0070\n",
      "     76        0.6581       0.5000        \u001b[35m0.6931\u001b[0m  0.0067\n",
      "     77        0.7192       0.5000        \u001b[35m0.6931\u001b[0m  0.0104\n",
      "     78        0.7026       0.5000        \u001b[35m0.6931\u001b[0m  0.0098\n",
      "     79        0.7158       0.5000        \u001b[35m0.6931\u001b[0m  0.0065\n",
      "     80        0.6979       0.5000        0.6931  0.0118\n",
      "     81        0.7410       0.5000        \u001b[35m0.6931\u001b[0m  0.0095\n",
      "     82        0.7244       0.5000        \u001b[35m0.6931\u001b[0m  0.0066\n",
      "     83        0.7550       0.5000        \u001b[35m0.6931\u001b[0m  0.0107\n",
      "     84        0.7951       0.5000        \u001b[35m0.6931\u001b[0m  0.0072\n",
      "     85        0.7229       0.5000        0.6931  0.0088\n",
      "     86        0.6763       0.5000        0.6931  0.0128\n",
      "     87        0.7650       0.5000        \u001b[35m0.6931\u001b[0m  0.0110\n",
      "     88        0.7654       0.5000        0.6931  0.0077\n",
      "     89        0.7280       0.5000        0.6931  0.0076\n",
      "     90        0.7321       0.5000        0.6931  0.0110\n",
      "     91        0.6994       0.5000        0.6931  0.0061\n",
      "     92        0.7845       0.5000        0.6931  0.0100\n",
      "     93        0.6848       0.5000        0.6931  0.0112\n",
      "     94        0.7239       0.5000        0.6931  0.0130\n",
      "     95        0.7249       0.5000        0.6931  0.0096\n",
      "     96        0.7749       0.5000        0.6931  0.0078\n",
      "     97        0.6518       0.5000        0.6931  0.0084\n",
      "     98        0.7710       0.5000        0.6931  0.0073\n",
      "     99        0.6791       0.5000        0.6931  0.0116\n",
      "    100        0.6953       0.5000        0.6931  0.0083\n",
      "    101        0.6896       0.5000        0.6931  0.0065\n",
      "    102        0.7170       0.5000        0.6931  0.0077\n",
      "    103        0.6947       0.5000        0.6931  0.0080\n",
      "    104        0.7427       0.5000        0.6931  0.0080\n",
      "    105        0.8014       0.5000        0.6931  0.0090\n",
      "    106        0.6914       0.5000        0.6931  0.0080\n",
      "    107        0.7150       0.5000        0.6931  0.0092\n",
      "    108        0.7232       0.5000        0.6931  0.0105\n",
      "    109        0.6514       0.5000        0.6931  0.0076\n",
      "    110        0.7675       0.5000        0.6931  0.0077\n",
      "    111        0.7852       0.5000        0.6931  0.0081\n",
      "    112        0.6864       0.5000        0.6931  0.0088\n",
      "    113        0.7327       0.5000        0.6932  0.0090\n",
      "    114        0.7318       0.5000        0.6932  0.0057\n",
      "    115        0.7218       0.5000        0.6932  0.0070\n",
      "    116        0.7339       0.5000        0.6932  0.0083\n",
      "    117        0.6934       0.5000        0.6932  0.0080\n",
      "    118        0.6926       0.5000        0.6932  0.0101\n",
      "    119        0.6669       0.5000        0.6932  0.0122\n",
      "    120        0.7895       0.5000        0.6932  0.0063\n",
      "    121        0.7360       0.5000        0.6932  0.0075\n",
      "    122        0.7070       0.5000        0.6932  0.0061\n",
      "    123        0.7256       0.5000        0.6932  0.0085\n",
      "    124        0.7705       0.5000        0.6932  0.0090\n",
      "    125        0.7052       0.5000        0.6932  0.0075\n",
      "    126        0.7018       0.5000        0.6932  0.0097\n",
      "    127        0.6779       0.5000        0.6932  0.0111\n",
      "    128        0.7723       0.5000        0.6932  0.0078\n",
      "    129        0.7693       0.5000        0.6932  0.0081\n",
      "    130        0.6365       0.5000        0.6932  0.0088\n",
      "    131        0.7420       0.5000        0.6932  0.0080\n",
      "    132        0.6870       0.5000        0.6932  0.0108\n",
      "    133        0.6986       0.5000        0.6932  0.0132\n",
      "    134        0.7707       0.5000        0.6932  0.0084\n",
      "    135        0.7500       0.5000        0.6932  0.0101\n",
      "    136        0.6913       0.5000        0.6932  0.0097\n",
      "    137        0.7393       0.5000        0.6932  0.0090\n",
      "    138        0.7299       0.5000        0.6932  0.0090\n",
      "    139        0.7371       0.5000        0.6932  0.0077\n",
      "    140        \u001b[36m0.6146\u001b[0m       0.5000        0.6932  0.0090\n",
      "    141        0.7390       0.5000        0.6932  0.0080\n",
      "    142        0.7364       0.5000        0.6932  0.0070\n",
      "    143        0.7197       0.5000        0.6932  0.0092\n",
      "    144        0.6628       0.5000        0.6932  0.0065\n",
      "    145        0.7708       0.5000        0.6932  0.0114\n",
      "    146        0.7221       0.5000        0.6932  0.0070\n",
      "    147        0.7049       0.5000        0.6932  0.0073\n",
      "    148        0.6733       0.5000        0.6932  0.0090\n",
      "    149        0.6739       0.5000        0.6932  0.0070\n",
      "    150        0.6866       0.5000        0.6932  0.0080\n",
      "    151        0.7486       0.5000        0.6932  0.0090\n",
      "    152        0.7820       0.5000        0.6932  0.0050\n",
      "    153        0.6865       0.5000        0.6932  0.0100\n",
      "    154        0.6587       0.5000        0.6932  0.0101\n",
      "    155        0.7097       0.5000        0.6932  0.0070\n",
      "    156        0.6994       0.5000        0.6932  0.0090\n",
      "    157        0.7379       0.5000        0.6932  0.0065\n",
      "    158        0.7981       0.5000        0.6932  0.0071\n",
      "    159        0.7407       0.5000        0.6932  0.0088\n",
      "    160        0.6819       0.5000        0.6932  0.0071\n",
      "    161        0.7064       0.5000        0.6932  0.0087\n",
      "    162        0.6793       0.5000        0.6932  0.0065\n",
      "    163        0.6624       0.5000        0.6932  0.0090\n",
      "    164        0.7183       0.5000        0.6932  0.0108\n",
      "    165        0.7081       0.5000        0.6932  0.0080\n",
      "    166        0.7359       0.5000        0.6932  0.0068\n",
      "    167        0.7585       0.5000        0.6932  0.0080\n",
      "    168        0.6677       0.5000        0.6932  0.0077\n",
      "    169        0.7210       0.5000        0.6932  0.0089\n",
      "    170        0.7131       0.5000        0.6932  0.0073\n",
      "    171        0.7308       0.5000        0.6932  0.0101\n",
      "    172        0.7012       0.5000        0.6932  0.0078\n",
      "    173        0.7190       0.5000        0.6932  0.0076\n",
      "    174        0.7107       0.5000        0.6932  0.0077\n",
      "    175        0.6759       0.5000        0.6932  0.0080\n",
      "    176        0.6838       0.5000        0.6932  0.0086\n",
      "    177        0.6782       0.5000        0.6932  0.0056\n",
      "    178        0.7783       0.5000        0.6932  0.0080\n",
      "    179        0.7546       0.5000        0.6932  0.0092\n",
      "    180        0.7283       0.5000        0.6932  0.0070\n",
      "    181        0.7226       0.5000        0.6932  0.0098\n",
      "    182        0.7194       0.5000        0.6932  0.0070\n",
      "    183        0.6914       0.5000        0.6932  0.0097\n",
      "    184        0.8147       0.5000        0.6932  0.0080\n",
      "    185        0.7383       0.5000        0.6932  0.0078\n",
      "    186        0.6586       0.5000        0.6932  0.0087\n",
      "    187        0.6708       0.5000        0.6932  0.0076\n",
      "    188        0.7220       0.5000        0.6932  0.0071\n",
      "    189        0.7123       0.5000        0.6932  0.0092\n",
      "    190        0.7841       0.5000        0.6932  0.0070\n",
      "    191        0.7738       0.5000        0.6932  0.0080\n",
      "    192        0.6898       0.5000        0.6932  0.0075\n",
      "    193        0.7703       0.5000        0.6932  0.0070\n",
      "    194        0.6941       0.5000        0.6932  0.0111\n",
      "    195        0.6681       0.5000        0.6932  0.0081\n",
      "    196        0.7413       0.5000        0.6932  0.0088\n",
      "    197        0.7506       0.5000        0.6932  0.0106\n",
      "    198        0.7331       0.5000        0.6932  0.0078\n",
      "    199        0.6821       0.5000        0.6932  0.0090\n",
      "    200        0.7131       0.5000        0.6932  0.0098\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline on the aptamer-protein pairs\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Predict the labels for the training data\n",
    "y_pred = pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "945dc876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Optional: Evaluate training accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_score(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyaptamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
