{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e8d179",
   "metadata": {},
   "source": [
    "# Binding prediction using AptaNet\n",
    "Step-by-step guide to using AptaNet for binary aptamer–protein binding prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a64683",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- **pairs_to_features**: converts `(aptamer_seq, protein_seq)` pairs into feature vectors using k-mer + PSeAAC.\n",
    "- **FeatureSelector**: a Random Forest-based transformer that selects important features.\n",
    "- **SkorchAptaNet**: a PyTorch MLP wrapped in Skorch for binary classification with a configurable threshold.\n",
    "- **load_pfoa_structure**: helper to load PFOA molecule structure from PDB file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a5052",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import the core functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eabec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torch  # noqa: I001\n",
    "\n",
    "# Data imports\n",
    "from pyaptamer.datasets import load_1gnh_structure\n",
    "from pyaptamer.utils.struct_to_aaseq import struct_to_aaseq\n",
    "\n",
    "# If you want to use the AptaNet pipeline, you can import it directly\n",
    "from pyaptamer.aptanet import AptaNetPipeline\n",
    "\n",
    "# If you to build your own aptamer pipeline, you should use the following imports\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from pyaptamer.utils._aptanet_utils import pairs_to_features\n",
    "from pyaptamer.aptanet._aptanet_nn import AptaNetMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c592d9",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "To train the skorch network the notebook uses:\n",
    "* As `X`:\n",
    "    * 5 random aptamer sequences of length>30 (to satisfy the default lambda value of 30 set in the PSeAAC algorithm).\n",
    "    * Amino-acid sequences extracted from the 1GNH protein molecule.\n",
    "* As `y`:\n",
    "    * A random binary value (0/1) equal to the number of `(aptamer_seq, protein_seq)` pairs as dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f6701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_sequence = [\n",
    "    \"GGGAGGACGAAGACGACUCGAGACAGGCUAGGGAGGGA\",\n",
    "    \"AAGCGUCGGAUCUACACGUGCGAUAGCUCAGUACGCGGU\",\n",
    "    \"CGGUAUCGAGUACAGGAGUCCGACGGAUAGUCCGGAGC\",\n",
    "    \"UAGCUAGCGAACUAGGCGUAGCUUCGAGUAGCUACGGAA\",\n",
    "    \"GCUAGGACGAUCGCACGUGACCGUCAGUAGCGUAGGAGA\",\n",
    "]\n",
    "\n",
    "gnh = load_1gnh_structure()\n",
    "protein_sequence = struct_to_aaseq(gnh)\n",
    "\n",
    "# Build all combinations (aptamer, protein)\n",
    "X = [(a, p) for a in aptamer_sequence for p in protein_sequence]\n",
    "\n",
    "# Dummy binary labels for the pairs\n",
    "y = torch.randint(0, 2, (len(X),), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd459190",
   "metadata": {},
   "source": [
    "## Build your own pipeline\n",
    " To build a scikit-learn pipeline, follow these steps:\n",
    "1. Convert the input to the desired (aptamer_sequence, protein_sequence) format.\n",
    "    * OPTIONAL: As mentioned in the paper, perform under-sampling using the  \n",
    "    Neighborhood Cleaning Rule to balance the classes.\n",
    "2. Get the PSeAAC feature vectors for your converted input (using `pairs_to_features`).\n",
    "3. Select the number of features to use from the feature vector (using `FeatureSelector`).\n",
    "4. Define the skorch neural network (using `AptaNetMLP`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ae4ea",
   "metadata": {},
   "source": [
    "## Different workflows\n",
    "In this notebook we will cover 2 different workflows one can follow, in ascending order of customizability:\n",
    "\n",
    "1. A minimal workflow with no dataset balancing, while using the in-built pipeline.\n",
    "3. Dataset balancing using under-sampling, while using your own pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c1997",
   "metadata": {},
   "source": [
    "### First workflow\n",
    "A minimal workflow with no dataset balancing, while using the in-built pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d782bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AptaNetPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db693e",
   "metadata": {},
   "source": [
    "### Second workflow\n",
    "Dataset balancing using under-sampling, while using your own pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: If you want to use the Neighborhood Cleaning Rule for under-sampling\n",
    "# %pip install imblearn\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you want to use the under-sampling, you need to install imbalanced-learn\n",
    "# and use the Pipeline from imblearn\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = AptaNetPipeline()\n",
    "\n",
    "feature_transformer = FunctionTransformer(\n",
    "    func=pairs_to_features,\n",
    "    validate=False,\n",
    "    # Optional arguments for pairs_to_features\n",
    "    # example: kw_args={'k': 4, 'pseaac_kwargs': {'lambda_value': 30}}\n",
    "    kw_args={},\n",
    ")\n",
    "\n",
    "selector = SelectFromModel(\n",
    "    estimator=RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=9,\n",
    "        random_state=None,\n",
    "    ),\n",
    "    threshold=\"mean\",\n",
    ")\n",
    "\n",
    "# Define the classifier\n",
    "net = NeuralNetBinaryClassifier(\n",
    "    module=AptaNetMLP,\n",
    "    module__input_dim=None,\n",
    "    module__hidden_dim=128,\n",
    "    module__n_hidden=7,\n",
    "    module__dropout=0.3,\n",
    "    module__output_dim=1,\n",
    "    module__use_lazy=True,\n",
    "    criterion=torch.nn.BCEWithLogitsLoss,\n",
    "    max_epochs=200,\n",
    "    lr=0.00014,\n",
    "    optimizer=torch.optim.RMSprop,\n",
    "    optimizer__alpha=0.9,\n",
    "    optimizer__eps=1e-08,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"features\", feature_transformer),\n",
    "        # Optional under-sampling, use sklearn's Pipeline if you do not need it\n",
    "        (\"ncr\", NeighbourhoodCleaningRule()),\n",
    "        (\"selector\", selector),\n",
    "        (\"clf\", net),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70138a",
   "metadata": {},
   "source": [
    "## Model Training and Prediction\n",
    "\n",
    "Now that we’ve defined our AptaNet pipeline, we proceed to train the model, and use it to predict, on our aptamer-protein dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed76399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7270\u001b[0m       \u001b[32m0.2857\u001b[0m        \u001b[35m0.7139\u001b[0m  0.1351\n",
      "      2        0.7593       0.2857        \u001b[35m0.7133\u001b[0m  0.0065\n",
      "      3        \u001b[36m0.6690\u001b[0m       0.2857        \u001b[35m0.7128\u001b[0m  0.0060\n",
      "      4        0.7504       0.2857        \u001b[35m0.7124\u001b[0m  0.0070\n",
      "      5        0.7646       0.2857        \u001b[35m0.7121\u001b[0m  0.0065\n",
      "      6        0.7945       0.2857        \u001b[35m0.7117\u001b[0m  0.0075\n",
      "      7        \u001b[36m0.6541\u001b[0m       0.2857        \u001b[35m0.7114\u001b[0m  0.0052\n",
      "      8        0.7842       0.2857        \u001b[35m0.7111\u001b[0m  0.0075\n",
      "      9        \u001b[36m0.6305\u001b[0m       0.2857        \u001b[35m0.7107\u001b[0m  0.0065\n",
      "     10        0.7223       0.2857        \u001b[35m0.7104\u001b[0m  0.0075\n",
      "     11        0.6717       0.2857        \u001b[35m0.7101\u001b[0m  0.0075\n",
      "     12        0.7168       0.2857        \u001b[35m0.7098\u001b[0m  0.0085\n",
      "     13        0.7049       0.2857        \u001b[35m0.7095\u001b[0m  0.0084\n",
      "     14        0.6947       0.2857        \u001b[35m0.7093\u001b[0m  0.0060\n",
      "     15        0.7417       0.2857        \u001b[35m0.7090\u001b[0m  0.0060\n",
      "     16        0.7223       0.2857        \u001b[35m0.7088\u001b[0m  0.0080\n",
      "     17        0.7022       0.2857        \u001b[35m0.7086\u001b[0m  0.0080\n",
      "     18        0.7069       0.2857        \u001b[35m0.7082\u001b[0m  0.0070\n",
      "     19        0.7368       0.2857        \u001b[35m0.7079\u001b[0m  0.0060\n",
      "     20        0.7215       0.2857        \u001b[35m0.7077\u001b[0m  0.0061\n",
      "     21        0.6639       0.2857        \u001b[35m0.7075\u001b[0m  0.0055\n",
      "     22        0.7287       0.2857        \u001b[35m0.7072\u001b[0m  0.0065\n",
      "     23        0.7406       0.2857        \u001b[35m0.7069\u001b[0m  0.0060\n",
      "     24        0.7040       0.2857        \u001b[35m0.7067\u001b[0m  0.0065\n",
      "     25        0.7314       0.2857        \u001b[35m0.7065\u001b[0m  0.0065\n",
      "     26        0.6774       0.2857        \u001b[35m0.7062\u001b[0m  0.0050\n",
      "     27        0.7462       0.2857        \u001b[35m0.7059\u001b[0m  0.0060\n",
      "     28        0.7450       0.2857        \u001b[35m0.7056\u001b[0m  0.0070\n",
      "     29        0.6780       0.2857        \u001b[35m0.7054\u001b[0m  0.0103\n",
      "     30        0.7395       0.2857        \u001b[35m0.7052\u001b[0m  0.0060\n",
      "     31        0.7382       0.2857        \u001b[35m0.7049\u001b[0m  0.0060\n",
      "     32        0.6924       0.2857        \u001b[35m0.7047\u001b[0m  0.0060\n",
      "     33        0.6671       0.2857        \u001b[35m0.7044\u001b[0m  0.0055\n",
      "     34        0.7636       0.2857        \u001b[35m0.7042\u001b[0m  0.0050\n",
      "     35        0.7037       0.2857        \u001b[35m0.7040\u001b[0m  0.0060\n",
      "     36        0.7202       0.2857        \u001b[35m0.7037\u001b[0m  0.0060\n",
      "     37        0.7266       0.2857        \u001b[35m0.7034\u001b[0m  0.0055\n",
      "     38        0.6747       0.2857        \u001b[35m0.7032\u001b[0m  0.0055\n",
      "     39        0.7439       0.2857        \u001b[35m0.7031\u001b[0m  0.0060\n",
      "     40        0.7063       0.2857        \u001b[35m0.7028\u001b[0m  0.0060\n",
      "     41        0.7406       0.2857        \u001b[35m0.7026\u001b[0m  0.0072\n",
      "     42        0.6969       0.2857        \u001b[35m0.7024\u001b[0m  0.0065\n",
      "     43        0.6963       0.2857        \u001b[35m0.7022\u001b[0m  0.0089\n",
      "     44        0.6551       0.2857        \u001b[35m0.7020\u001b[0m  0.0075\n",
      "     45        0.7196       0.2857        \u001b[35m0.7017\u001b[0m  0.0055\n",
      "     46        0.6923       0.2857        \u001b[35m0.7015\u001b[0m  0.0065\n",
      "     47        0.7086       0.2857        \u001b[35m0.7012\u001b[0m  0.0060\n",
      "     48        0.7764       0.2857        \u001b[35m0.7009\u001b[0m  0.0055\n",
      "     49        0.6817       0.2857        \u001b[35m0.7007\u001b[0m  0.0056\n",
      "     50        0.7038       0.2857        \u001b[35m0.7005\u001b[0m  0.0070\n",
      "     51        0.7969       0.2857        \u001b[35m0.7003\u001b[0m  0.0060\n",
      "     52        0.6521       0.2857        \u001b[35m0.7000\u001b[0m  0.0060\n",
      "     53        0.6802       0.2857        \u001b[35m0.6998\u001b[0m  0.0080\n",
      "     54        0.7050       0.2857        \u001b[35m0.6996\u001b[0m  0.0054\n",
      "     55        0.7169       0.2857        \u001b[35m0.6993\u001b[0m  0.0065\n",
      "     56        \u001b[36m0.6261\u001b[0m       0.2857        \u001b[35m0.6990\u001b[0m  0.0055\n",
      "     57        0.6800       0.2857        \u001b[35m0.6988\u001b[0m  0.0065\n",
      "     58        0.7033       0.2857        \u001b[35m0.6986\u001b[0m  0.0060\n",
      "     59        0.6895       0.2857        \u001b[35m0.6983\u001b[0m  0.0063\n",
      "     60        0.6520       0.2857        \u001b[35m0.6980\u001b[0m  0.0066\n",
      "     61        0.7063       0.2857        \u001b[35m0.6978\u001b[0m  0.0072\n",
      "     62        0.6859       0.2857        \u001b[35m0.6975\u001b[0m  0.0075\n",
      "     63        0.6949       0.2857        \u001b[35m0.6972\u001b[0m  0.0115\n",
      "     64        0.6953       0.2857        \u001b[35m0.6970\u001b[0m  0.0090\n",
      "     65        0.6605       0.2857        \u001b[35m0.6968\u001b[0m  0.0105\n",
      "     66        0.6813       0.2857        \u001b[35m0.6965\u001b[0m  0.0095\n",
      "     67        0.6431       0.2857        \u001b[35m0.6962\u001b[0m  0.0065\n",
      "     68        0.7169       0.2857        \u001b[35m0.6960\u001b[0m  0.0066\n",
      "     69        0.6453       0.2857        \u001b[35m0.6957\u001b[0m  0.0087\n",
      "     70        0.7232       0.2857        \u001b[35m0.6954\u001b[0m  0.0085\n",
      "     71        0.6722       0.2857        \u001b[35m0.6952\u001b[0m  0.0065\n",
      "     72        0.6674       0.2857        \u001b[35m0.6949\u001b[0m  0.0065\n",
      "     73        \u001b[36m0.5985\u001b[0m       0.2857        \u001b[35m0.6946\u001b[0m  0.0060\n",
      "     74        0.7594       0.2857        \u001b[35m0.6944\u001b[0m  0.0080\n",
      "     75        0.7106       0.2857        \u001b[35m0.6940\u001b[0m  0.0070\n",
      "     76        0.6292       0.2857        \u001b[35m0.6937\u001b[0m  0.0070\n",
      "     77        0.7079       0.2857        \u001b[35m0.6933\u001b[0m  0.0065\n",
      "     78        0.6539       \u001b[32m0.7143\u001b[0m        \u001b[35m0.6930\u001b[0m  0.0075\n",
      "     79        0.6243       0.7143        \u001b[35m0.6927\u001b[0m  0.0102\n",
      "     80        0.6277       0.7143        \u001b[35m0.6924\u001b[0m  0.0075\n",
      "     81        0.6711       0.7143        \u001b[35m0.6921\u001b[0m  0.0075\n",
      "     82        0.6767       0.7143        \u001b[35m0.6918\u001b[0m  0.0096\n",
      "     83        0.6645       0.7143        \u001b[35m0.6916\u001b[0m  0.0070\n",
      "     84        \u001b[36m0.5840\u001b[0m       0.7143        \u001b[35m0.6913\u001b[0m  0.0075\n",
      "     85        0.6084       0.7143        \u001b[35m0.6910\u001b[0m  0.0085\n",
      "     86        0.6562       0.7143        \u001b[35m0.6907\u001b[0m  0.0065\n",
      "     87        0.6128       0.7143        \u001b[35m0.6905\u001b[0m  0.0075\n",
      "     88        0.6835       0.7143        \u001b[35m0.6901\u001b[0m  0.0065\n",
      "     89        0.6755       0.7143        \u001b[35m0.6898\u001b[0m  0.0055\n",
      "     90        0.6373       0.7143        \u001b[35m0.6894\u001b[0m  0.0085\n",
      "     91        0.6462       0.7143        \u001b[35m0.6891\u001b[0m  0.0095\n",
      "     92        0.6732       0.7143        \u001b[35m0.6888\u001b[0m  0.0065\n",
      "     93        0.6259       0.7143        \u001b[35m0.6885\u001b[0m  0.0060\n",
      "     94        \u001b[36m0.5713\u001b[0m       0.7143        \u001b[35m0.6882\u001b[0m  0.0060\n",
      "     95        0.6614       0.7143        \u001b[35m0.6879\u001b[0m  0.0070\n",
      "     96        0.6827       0.7143        \u001b[35m0.6876\u001b[0m  0.0065\n",
      "     97        0.6895       0.7143        \u001b[35m0.6873\u001b[0m  0.0065\n",
      "     98        0.6384       0.7143        \u001b[35m0.6870\u001b[0m  0.0075\n",
      "     99        0.6474       0.7143        \u001b[35m0.6866\u001b[0m  0.0065\n",
      "    100        0.6510       0.7143        \u001b[35m0.6863\u001b[0m  0.0065\n",
      "    101        0.6880       0.7143        \u001b[35m0.6860\u001b[0m  0.0085\n",
      "    102        0.6318       0.7143        \u001b[35m0.6856\u001b[0m  0.0060\n",
      "    103        0.6712       0.7143        \u001b[35m0.6854\u001b[0m  0.0070\n",
      "    104        0.6565       0.7143        \u001b[35m0.6850\u001b[0m  0.0076\n",
      "    105        0.7100       0.7143        \u001b[35m0.6847\u001b[0m  0.0065\n",
      "    106        0.6256       0.7143        \u001b[35m0.6844\u001b[0m  0.0105\n",
      "    107        0.6570       0.7143        \u001b[35m0.6841\u001b[0m  0.0081\n",
      "    108        0.6010       0.7143        \u001b[35m0.6838\u001b[0m  0.0070\n",
      "    109        0.7151       0.7143        \u001b[35m0.6835\u001b[0m  0.0075\n",
      "    110        0.6056       0.7143        \u001b[35m0.6832\u001b[0m  0.0065\n",
      "    111        0.6335       0.7143        \u001b[35m0.6830\u001b[0m  0.0065\n",
      "    112        0.6703       0.7143        \u001b[35m0.6828\u001b[0m  0.0075\n",
      "    113        0.6782       0.7143        \u001b[35m0.6824\u001b[0m  0.0060\n",
      "    114        0.5827       0.7143        \u001b[35m0.6821\u001b[0m  0.0060\n",
      "    115        0.6917       0.7143        \u001b[35m0.6817\u001b[0m  0.0065\n",
      "    116        0.6946       0.7143        \u001b[35m0.6815\u001b[0m  0.0065\n",
      "    117        0.6168       0.7143        \u001b[35m0.6811\u001b[0m  0.0095\n",
      "    118        0.6087       0.7143        \u001b[35m0.6807\u001b[0m  0.0075\n",
      "    119        0.6308       0.7143        \u001b[35m0.6804\u001b[0m  0.0096\n",
      "    120        0.6763       0.7143        \u001b[35m0.6800\u001b[0m  0.0075\n",
      "    121        0.6419       0.7143        \u001b[35m0.6797\u001b[0m  0.0086\n",
      "    122        0.6456       0.7143        \u001b[35m0.6794\u001b[0m  0.0073\n",
      "    123        0.6694       0.7143        \u001b[35m0.6790\u001b[0m  0.0075\n",
      "    124        0.6376       0.7143        \u001b[35m0.6788\u001b[0m  0.0080\n",
      "    125        0.5772       0.7143        \u001b[35m0.6784\u001b[0m  0.0071\n",
      "    126        0.6383       0.7143        \u001b[35m0.6781\u001b[0m  0.0095\n",
      "    127        0.6252       0.7143        \u001b[35m0.6777\u001b[0m  0.0105\n",
      "    128        0.6059       0.7143        \u001b[35m0.6774\u001b[0m  0.0115\n",
      "    129        0.6552       0.7143        \u001b[35m0.6770\u001b[0m  0.0105\n",
      "    130        0.6328       0.7143        \u001b[35m0.6767\u001b[0m  0.0115\n",
      "    131        0.7602       0.7143        \u001b[35m0.6764\u001b[0m  0.0092\n",
      "    132        0.6318       0.7143        \u001b[35m0.6762\u001b[0m  0.0095\n",
      "    133        0.6550       0.7143        \u001b[35m0.6758\u001b[0m  0.0085\n",
      "    134        0.6072       0.7143        \u001b[35m0.6754\u001b[0m  0.0077\n",
      "    135        0.6381       0.7143        \u001b[35m0.6751\u001b[0m  0.0085\n",
      "    136        0.6897       0.7143        \u001b[35m0.6747\u001b[0m  0.0114\n",
      "    137        0.6703       0.7143        \u001b[35m0.6744\u001b[0m  0.0070\n",
      "    138        0.6498       0.7143        \u001b[35m0.6739\u001b[0m  0.0081\n",
      "    139        0.6142       0.7143        \u001b[35m0.6736\u001b[0m  0.0063\n",
      "    140        0.6197       0.7143        \u001b[35m0.6732\u001b[0m  0.0076\n",
      "    141        0.6230       0.7143        \u001b[35m0.6727\u001b[0m  0.0055\n",
      "    142        \u001b[36m0.5150\u001b[0m       0.7143        \u001b[35m0.6723\u001b[0m  0.0086\n",
      "    143        0.6590       0.7143        \u001b[35m0.6721\u001b[0m  0.0064\n",
      "    144        0.6753       0.7143        \u001b[35m0.6717\u001b[0m  0.0065\n",
      "    145        0.6187       0.7143        \u001b[35m0.6714\u001b[0m  0.0050\n",
      "    146        0.6524       0.7143        \u001b[35m0.6710\u001b[0m  0.0066\n",
      "    147        0.6239       0.7143        \u001b[35m0.6706\u001b[0m  0.0095\n",
      "    148        0.5996       0.7143        \u001b[35m0.6702\u001b[0m  0.0055\n",
      "    149        0.7024       0.7143        \u001b[35m0.6696\u001b[0m  0.0065\n",
      "    150        0.7277       0.7143        \u001b[35m0.6692\u001b[0m  0.0055\n",
      "    151        0.5925       0.7143        \u001b[35m0.6688\u001b[0m  0.0065\n",
      "    152        0.6529       0.7143        \u001b[35m0.6685\u001b[0m  0.0060\n",
      "    153        0.6790       0.7143        \u001b[35m0.6681\u001b[0m  0.0075\n",
      "    154        0.6155       0.7143        \u001b[35m0.6677\u001b[0m  0.0065\n",
      "    155        0.6151       0.7143        \u001b[35m0.6673\u001b[0m  0.0075\n",
      "    156        0.6907       0.7143        \u001b[35m0.6669\u001b[0m  0.0063\n",
      "    157        0.5961       0.7143        \u001b[35m0.6664\u001b[0m  0.0065\n",
      "    158        0.6589       0.7143        \u001b[35m0.6662\u001b[0m  0.0075\n",
      "    159        0.6698       0.7143        \u001b[35m0.6658\u001b[0m  0.0085\n",
      "    160        0.5743       0.7143        \u001b[35m0.6654\u001b[0m  0.0055\n",
      "    161        0.6840       0.7143        \u001b[35m0.6651\u001b[0m  0.0070\n",
      "    162        0.6031       0.7143        \u001b[35m0.6648\u001b[0m  0.0070\n",
      "    163        0.6313       0.7143        \u001b[35m0.6644\u001b[0m  0.0070\n",
      "    164        0.6370       0.7143        \u001b[35m0.6639\u001b[0m  0.0060\n",
      "    165        0.6409       0.7143        \u001b[35m0.6636\u001b[0m  0.0103\n",
      "    166        0.6887       0.7143        \u001b[35m0.6634\u001b[0m  0.0091\n",
      "    167        0.5904       0.7143        \u001b[35m0.6628\u001b[0m  0.0080\n",
      "    168        0.6233       0.7143        \u001b[35m0.6623\u001b[0m  0.0070\n",
      "    169        0.6182       0.7143        \u001b[35m0.6619\u001b[0m  0.0082\n",
      "    170        0.6226       0.7143        \u001b[35m0.6616\u001b[0m  0.0070\n",
      "    171        0.6712       0.7143        \u001b[35m0.6611\u001b[0m  0.0070\n",
      "    172        0.5928       0.7143        \u001b[35m0.6607\u001b[0m  0.0080\n",
      "    173        0.6135       0.7143        \u001b[35m0.6603\u001b[0m  0.0095\n",
      "    174        0.6226       0.7143        \u001b[35m0.6598\u001b[0m  0.0095\n",
      "    175        0.6370       0.7143        \u001b[35m0.6594\u001b[0m  0.0096\n",
      "    176        0.6274       0.7143        \u001b[35m0.6590\u001b[0m  0.0075\n",
      "    177        0.6356       0.7143        \u001b[35m0.6587\u001b[0m  0.0065\n",
      "    178        0.5901       0.7143        \u001b[35m0.6582\u001b[0m  0.0075\n",
      "    179        0.5951       0.7143        \u001b[35m0.6580\u001b[0m  0.0105\n",
      "    180        0.6393       0.7143        \u001b[35m0.6575\u001b[0m  0.0111\n",
      "    181        0.6670       0.7143        \u001b[35m0.6571\u001b[0m  0.0072\n",
      "    182        0.5908       0.7143        \u001b[35m0.6567\u001b[0m  0.0085\n",
      "    183        0.6036       0.7143        \u001b[35m0.6563\u001b[0m  0.0074\n",
      "    184        0.6216       0.7143        \u001b[35m0.6559\u001b[0m  0.0065\n",
      "    185        0.6546       0.7143        \u001b[35m0.6555\u001b[0m  0.0084\n",
      "    186        0.6552       0.7143        \u001b[35m0.6551\u001b[0m  0.0065\n",
      "    187        0.6021       0.7143        \u001b[35m0.6546\u001b[0m  0.0075\n",
      "    188        0.6297       0.7143        \u001b[35m0.6542\u001b[0m  0.0065\n",
      "    189        0.6158       0.7143        \u001b[35m0.6537\u001b[0m  0.0075\n",
      "    190        0.5664       0.7143        \u001b[35m0.6532\u001b[0m  0.0060\n",
      "    191        0.5799       0.7143        \u001b[35m0.6528\u001b[0m  0.0070\n",
      "    192        0.6279       0.7143        \u001b[35m0.6524\u001b[0m  0.0062\n",
      "    193        0.5838       0.7143        \u001b[35m0.6520\u001b[0m  0.0073\n",
      "    194        0.5488       0.7143        \u001b[35m0.6516\u001b[0m  0.0055\n",
      "    195        0.6394       0.7143        \u001b[35m0.6511\u001b[0m  0.0065\n",
      "    196        0.6153       0.7143        \u001b[35m0.6508\u001b[0m  0.0075\n",
      "    197        0.6353       0.7143        \u001b[35m0.6505\u001b[0m  0.0060\n",
      "    198        0.6536       0.7143        \u001b[35m0.6500\u001b[0m  0.0063\n",
      "    199        0.6070       0.7143        \u001b[35m0.6497\u001b[0m  0.0055\n",
      "    200        0.5572       0.7143        \u001b[35m0.6493\u001b[0m  0.0065\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline on the aptamer-protein pairs\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Predict the labels for the training data\n",
    "y_pred = pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "945dc876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Optional: Evaluate training accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_score(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyaptamer-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
